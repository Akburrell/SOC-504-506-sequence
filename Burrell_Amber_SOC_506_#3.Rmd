---
output:
  pdf_document: default
  html_document: default
---
## **Homework 3**
## Amber Burrell


Spring 2022, Soc 506

Professor Tyler McCormick 

--------------------------------------------------
**Note:** Homework should be submitted via Canvas.  Must include both compiled results (pdf preferred) and codes (.Rmd file) for credit.


******

1. Let's look at data from the National Longitudinal Survey (NLS).  This is a major labor market survey conducted by the Bureau of Labor Statistics.  It’s a longitudinal survey, so participants were asked surveyed between 1968 and 1988 and each person is identified by an unique *idcode*. Load the data using the following code. 
```{r eval=TRUE, echo=TRUE}
#install.packages("webuse")
library(webuse)
nlswork_orig <- webuse('nlswork')
```
+ Perform exploratory data analysis.  As part of this analysis, examine patterns in missing data (not fishing for anything here, just for practice).

the original exploritory corrlation graph were all printing off with either NA, ? or with blanks. There seem to be so many NA in the data that it was corrupting its ability to calculate any of the correlations. I think that the fact that there are over 21 variable with over 25,000 different observations, it should not come as a surprise that there is a lot of missing data. After I narrowed down my correlations graphs I was able to get a much more comprehensive of how the selected variable relate to one another. I think these specific variables have probably the lower amount of NA which is why it could come up with correlation numbers.
```{r, cache=TRUE}

#install.packages("PerformanceAnalytics")
#install.packages("GGally")
#install.packages("ggthemes")
library(GGally)
library(ggthemes)
ggpairs(nlswork_orig[, c("age","ln_wage", "tenure", "union")], lower = list(continuous = wrap("smooth", alpha = 0.5, size = 0.2))) +theme_tufte()
#install.packages("corrplot")
library(corrplot)
ggcorr(nlswork_orig[, c("age","ln_wage", "tenure", "union")], geom = "circle", min_size = 10, max_size = 10,
label = TRUE, label_alpha = TRUE, label_round = 2, label_size = 3,
hjust = 1, layout.exp = 2)

summary(nlswork_orig)



#install.packages("gplots")
```


+ Fit a regression model to investigate the relationship between wage (here as log-scaled GNP-adjusted wage) as dependent variable ``ln_wage`` and survey participant’s current ``age``, job ``tenure`` in years and ``union`` membership as independent variables.
```{r}
Model1<- glm(ln_wage~1 + age + tenure + union, family= gaussian(link='identity'), data = nlswork_orig)
summary(Model1)

```


+ Perform model diagnostics and check the regression assumptions.  Are any assumptions violated?

based on the graph it does not look like there is a lot of variation in residual, However it does seem like there is a large portion of data in the lower predicted values than in the larger one. There are also not that many out lyers. I would say that clustering might help or some to just help even out the data rather than having a majory being in the begining. I just seems like there could be a reason for that. and analyzing the normal Q-Q, you can see that it does not exactly fit the linear line. 
```{r}
plot(Model1,which = c(1,2))
```


+ Our data contains repeated measures for each subject, so we have panel data in which each subject forms a group or cluster.  We can use a fixed-effects (FE) model to account for unobserved subject-specific characteristics. A FE model uses indicator variables for each group or cluster (in this case person) as a way of accounting for group-specific (here person-specific) structure not accounted for by the other covariates.

  + Transform the ``idcode`` variable into a factor (hint: ``idcode = as.factor(idcode)``)
```{r}
idcode = as.factor(nlswork_orig$idcode)
```

  + Make a table of the number of observations in the dataset per person. 
```{r}
library(dplyr)
my_summary_data <- nlswork_orig %>%
    group_by(idcode) %>%
    summarise(Count = n()) 
my_summary_data
```

  + As you see, there are a *lot* of people in the dataset, so running the analysis would involve almost 5,000 fixed effects! This will take a long time to run, so instead let's look only at people with ``idcode`` variables greater than 4000.  Filter the data with ``dplyr``.
  
```{r}
library(dplyr)
nlswork_orig2 <- filter(nlswork_orig, as.numeric(idcode) > 4000)  
```
  
  + Using this subset of the data fit a regression model that includes age, tenure, union membership, an interaction term for age and membership, and fixed effects for each person.
```{r}
Model2 <- lm(ln_wage~1 + age + tenure + union+ I(age*union) + factor(idcode)-1 , data = nlswork_orig2)
summary(Model2)
```
  

  + Check the appropriate model diagnostics.
  
  This graph is 10x better. Here there isnt nearly as much data. It is also no so front heavy and the data is more equaly distributed. I will say the normal Q-Q plot could look better, but i think Model 2 is overal a much better fit for this data analysis.
```{r}
plot(Model2, which = c(1,2))
```
  

+ The above procedure introduces fixed effects to adjust the mean of the regression model person-specific effects.  But what about the variance? Use the ``modelsummary`` package to create (i) robust standard errors (ii) clustered standard errors where errors are clustered at the person level and (iii) clustered standard errors where errors are clustered by age.

```{r}
#install.packages("modelsummary")
library(modelsummary)
modelsummary(Model2,vcov = list("iid", "robust", ~idcode, ~age), 
             coef_omit = "idcode")
```

  + Read this [blog post](https://blogs.worldbank.org/impactevaluations/when-should-you-cluster-standard-errors-new-wisdom-econometrics-oracle) and discuss the implications of using (ii) versus (iii) above.


******
2. Practice with data wrangling.  This exercise is inspired by one written by Andrew Jaffe and John Muschelli (JHU).

+ Read the data provided using read_csv() and name it ``mort``. These data provide country-level under-five mortality estimates.  As we discussed in class, consider the patterns in missingness in the data anytime you think about analysis! Rename the first column to country using the ``rename()`` command in ``dplyr``. Create an object year variable by extracting column names (using ``colnames()``) and make it to an integer ``as.integer()`` ), excluding the first column either with string manipulations or bracket subsetting or subsetting with ``is.na()``.
```{r}
library(tidyverse)
library(rlang)
library(readxl)
library(dplyr)
mort = read_csv("http://johnmuschelli.com/intro_to_r/data/indicatordeadkids35.csv")
mort = mort %>% 
  rename(country = ...1)
year = colnames(mort)
year = as.integer(year)

```

+ Reshape the data so that there is a variable named year corresponding to year (key) and a column of the mortalities named mortality (value), using the ``tidyr`` package and its ``gather()`` function. Name the output long and make year a numeric variable. *Hint*: remember that -COLUMN_NAME removes that column, gather all the columns but country.

```{r}
library(tidyr)
# can use quotes
long = mort %>% 
  gather(key = "year", value = "mortality", -country)
long = long %>% 
  mutate(year = as.numeric(year))
```


+ Read in [this](http://johnmuschelli.com/intro_to_r/data/country_pop.txt) the tab-delim file and call it ``pop``. The file contains population information on each country. Rename the second column to "Country" and the column "% of world population", to percent. *Hint*: use read_tsv()

```{r}
library(readr)
pop <- read_tsv("http://johnmuschelli.com/intro_to_r/data/country_pop.txt")
pop = pop %>% 
  rename(Country = `Country (or dependent territory)`,
         percent = `% of world population`)

```


+ Determine the population of each country in pop using ``arrange()``. Get the order of the countries based on this (first is the highest population), and extract that column and call it ``pop_levels``. Make a variable in the long data set named sorted that is the country variable coded as a factor based on ``pop_levels``.
```{r}
pop = pop %>% 
  arrange(desc(Population))
# this is sorted !
pop_levels = pop$Country
long = long %>% 
  mutate(sorted = factor(country, levels = pop_levels))
```


+ Parts a, b, and c below are only broken up here for clarity, but all three components can be addressed in one chunk of code/as one function, using ``%>%`` as necessary.

```{r}
long_sub = long %>% 
  filter(year >= 1975 & year <= 2010)
range(long_sub$year)
```

    + Subset long based on years 1975-2010, including 1975 and 2010 and call this long_sub using ``&`` or the ``between()`` function.
```{r}
long_sub = long %>% 
  filter(between(year, 1975, 2010))
range(long_sub$year)

```

    
    + Further subset long_sub for the following countries using ``dplyr::filter()`` and the ``%in%`` operator on the sorted country ``factor (sorted):c("Venezuela", "Bahrain", "Estonia", "Iran", "Thailand", "Chile",  "Western Sahara", "Azerbaijan", "Argentina", "Haiti")``.
    + Lastly, remove missing rows for mortality using ``filter()`` and ``is.na()``. *Hint*: Be sure to assign your final object created from a through c as ``long_sub`` so you can use it below.
```{r}
long_sub = long_sub %>% 
  filter(sorted %in% c("Venezuela", "Bahrain", "Estonia", "Iran", "Thailand", "Chile",  "Western Sahara", "Azerbaijan", "Argentina", "Haiti")) %>% 
  filter(!is.na(mortality))
```

+ Plotting: create “spaghetti”/line plots for the countries in ``long_sub``, using different colors for different countries, using sorted. The x-axis should be year, and the y-axis should be mortality. Make the plot using a.``qplot`` and b.``ggplot`` (bonus).
```{r}
qplot(year, y = mortality, data = long_sub, color = sorted, geom = "line")

```


******

3. Now, let's practice manipulating data.  This exercise is taken from the same authors that inspired the one above.  Let's do it directly this time.  Complete the exercise [here](https://johnmuschelli.com/intro_to_r/Manipulating_Data_in_R/lab/Manipulating_Data_in_R_Lab.Rmd).  The lecture notes may also be helpful and can be found [here](https://johnmuschelli.com/intro_to_r/Manipulating_Data_in_R/Manipulating_Data_in_R.pdf).


